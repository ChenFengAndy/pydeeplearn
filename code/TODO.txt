TODO:

  do some experiments with the momentum changing more to 0.9 when you get closer

  implment NeRelu

  incoming constraints on l2 norms

   try to increase the momentum in the RBM class

   rettry tanh at some point

  wake sleep for generation: this is important because I can then use
    more unlabelled data for the emotion recognition task
  fix the learning rate

  play around with the learning rate for RBMs and the momentum: best way to do this is via crossvalidation

  make all these possible parameters as arguments to the script and check if they work

  implement and perform cross validation

  actually urn the MNIST with pca and then train
  fix the mnist memory problem when you have too many things

  think of implementing early stopping for the net

  especially important: implement chanign on the learning rate by monitoring the error
  I think there is something like this in the guide

  implement the l2 norm update for dropout as suggested: this will probably make
    things a lot slower and adds a hyper paramter (to be left towards the end, after crossvalidation is implemented)



runnign experiments:
  edge05 with 20% fixed but only with 10.000 to see that stuff is working

  edge02 with rbm dropout
