TODO:

  do some experiments with the momentum changing more to 0.9 when you get closer

  implment NeRelu

   try to increase the momentum in the RBM class

   rettry tanh at some point

  wake sleep for generation: this is important because I can then use
    more unlabelled data for the emotion recognition task
  fix the learning rate

  play around with the learning rate for RBMs and the momentum: best way to do this is via crossvalidation

  make all these possible parameters as arguments to the script and check if they work

  implement and perform cross validation

  actually urn the MNIST with pca and then train
  fix the mnist memory problem when you have too many things

  think of implementing early stopping for the net

  especially important: implement chanign on the learning rate by monitoring the error
  I think there is something like this in the guide

  implement the l2 norm update for dropout as suggested: this will probably make
    things a lot slower and adds a hyper paramter (to be left towards the end, after crossvalidation is implemented)



runnign experiments:
  edge05 with 20% fixed but only with 10.000 to see that stuff is working

  edge02 with rbm dropout


WHAT HAPPENS IF YOU INCREASE THE DATA SIZE FOR THE SECOND RBMS BY USING DIFFERENT RECONSTRUCTIONS OF THE DATA? or like dropout, droping out from different sides of the data.


TODO: get a generative sample from the deep belief nets for MNIST, like shown here
https://class.coursera.org/neuralnets-2012-001/lecture/151 minute 05:431